# MLOps : note technique

Issue du *DevOps*, la d√©marche MLOps (*Machine Learning Operating System*) sert √† encadrer et automatiser les t√¢ches n√©cessaires √† l'√©laboration d'un mod√®le d'apprentissage automatique, en vue de pouvoir s√©curiser et r√©p√©ter ces √©tapes et monitorer ses performances, dans un objectif d'am√©lioration continue.

## D√©marche CI/CD

Le *continuous integration / continuous delivery* est au coeur du MLOps :
- l'**int√©gration continue** consiste √† d√©velopper dans un environnement cloisonn√©, √† versionner le code, automatiser la construction de l'environnement qui ex√©cute ce code (build), √† effectuer les tests unitaires et v√©rifier la qualit√© g√©n√©rale du code (Lint)
- le **d√©ploiement continu** est le fait d'ex√©cuter ce code au sein d'un environnement de production, √† effectuer des tests de production, √† surveiller et mesurer son ex√©cution (artefacts, m√©triques, br√®ches de s√©curit√©, etc.)

Les applications varient en fonction des projets mais la d√©marche est la m√™me : chercher √† **am√©liorer continuellement la qualit√©** du projet, que l'on peut r√©sumer par le terme japonais "***kaizen***".

## En pratique

Dans le cadre du projet actuel, voici une mise en oeuvre MLOps possible :
- mise en place d'un **conteneur de d√©veloppement** (e.g. *Docker* ou machine virtuelle)
- mise en place du **versioning** (e.g. *GitHub*, *Framagit*, *GitLab*, ...)
- travail dans un **environnement de d√©veloppement** d√©di√© au projet (e.g. *Python venv*)
- mise en place d'outils de **contr√¥le de qualit√© du code** (e.g. *Pylint*)
- mise en place d'outils de **formatage du code** (e.g. *Black*, *Flake8*)
- **r√©pertoriage des biblioth√®ques** utilis√©es dans un ou plusieurs fichiers (e.g. *Poetry* ou un simple fichier `requirements.txt` utilisable avec *Pip*)
- cr√©ation de **tests unitaires** pour chaque fonction utilis√©e (e.g. *Pytest* ou *Unittest*)
- cr√©ation de pipelines (avec *Scikit Learn Pipeline* par exemple)
  - un **pipeline de pr√©-traitement des donn√©es** pour uniformiser et r√©p√©ter rapidement toutes les √©tapes de pr√©-traitement (e.g. supprimer les outliers, faire le feature engineering, etc.)
  - un **pipeline d'entra√Ænement** des mod√®les** (e.g. pr√©-traiter les donn√©es d'entr√©e, les normaliser puis les r√©duire √† 2 dimensions et enfin entra√Æner un mod√®le d√©fini dessus), parfois regroup√© avec le pr√©-traitement
  - un **pipeline d'inf√©rence et / ou de production** (e.g. pr√©-traitement des entr√©es puis inf√©rence sur tel mod√®le entra√Æn√©)

Une √©tape importante est la mise en place d'**actions automatiques**, d√©clench√©es par un √©v√®nement (e.g. *GitHub Actions*).  
Ici, on imagine qu'un *push* vers le r√©pertoire de versioning d√©clenche certaines actions pour une mise en production :
- cr√©ation du **conteneur**
- **installation** des biblioth√®ques requises
- **tests** unitaires
- **contr√¥le qualit√©** du code
- **formatage** du code
- ex√©cution du code de **d√©ploiement** de l'API



<!-- üößüößüößüößüößüöß -->

Une fois en production, 

- **suivi de la performance des mod√®les** par des 
- 
- 



- GH Actions √† chaque push
- MLFlow
- Pr√©senter la conception concr√®te du syst√®me de suivi de la performance adapt√© au projet : les indicateurs et mesures √† mettre en oeuvre, les types d‚Äôalerte pr√©conis√©es (il n‚Äôest pas demand√© de le d√©velopper)
- Suivi ‚Üí surveillance ‚Äúmodel drift‚Äù & ‚Äúdata drift‚Äù (d√©finir rapidement) : voir evidentlyAI, Promotheus, ou Popmon
  - benchmark avec avantages / inconv des solut¬∞ (+ inclure prix)
- syst√®me de stockage d‚Äô√©v√©nements relatifs aux pr√©dictions r√©alis√©es par l‚ÄôAPI et une gestion d‚Äôalerte en cas de d√©gradation significative de la performance.
- Pr√©senter comment utiliser les outils envisag√©s pour mettre en oeuvre le syst√®me de suivi
