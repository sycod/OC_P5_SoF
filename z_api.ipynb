{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS & env\n",
    "import os\n",
    "import dill as pickle\n",
    "import logging\n",
    "\n",
    "# ML\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "# home made functions\n",
    "from src.scrap_and_clean import preprocess_doc\n",
    "from src.models import w2v_vect_data\n",
    "from src.models import lr_predict_tags\n",
    "\n",
    "\n",
    "# logging configuration (see all outputs, even DEBUG or INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk downloads\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:⚙️ Loading vectorizer...\n",
      "INFO:gensim.utils:loading Word2Vec object from models/w2v_cbow_vectorizer\n",
      "INFO:gensim.utils:loading wv recursively from models/w2v_cbow_vectorizer.wv.* with mmap=None\n",
      "INFO:gensim.utils:setting ignored attribute cum_table to None\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'fname': 'models/w2v_cbow_vectorizer', 'datetime': '2024-06-10T15:01:04.611649', 'gensim': '4.3.2', 'python': '3.11.6 (main, Mar 19 2024, 19:27:13) [GCC 11.4.0]', 'platform': 'Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35', 'event': 'loaded'}\n",
      "INFO:root:✅ Vectorizer loaded\n",
      "INFO:root:⚙️ Loading classifier...\n",
      "INFO:root:✅ Classifier loaded\n",
      "INFO:root:⚙️ Loading keep set...\n",
      "INFO:root:✅ Keep set loaded\n",
      "INFO:root:⚙️ Loading exclude set...\n",
      "INFO:root:✅ Exclude set loaded\n"
     ]
    }
   ],
   "source": [
    "VECTORIZER_URI = \"models/w2v_cbow_vectorizer\"\n",
    "CLASSIFIER_URI = \"models/w2v_cbow_lrovr_classifier.pkl\"\n",
    "KEEP_SET_URI = \"data/keep_set.pkl\"\n",
    "EXCLUDE_SET_URI = \"data/exclude_set.pkl\"\n",
    "\n",
    "# load vectorizer\n",
    "logging.info(f\"⚙️ Loading vectorizer...\")\n",
    "if os.path.exists(VECTORIZER_URI):\n",
    "    vectorizer = Word2Vec.load(VECTORIZER_URI)\n",
    "    logging.info(f\"✅ Vectorizer loaded\")\n",
    "else:\n",
    "    logging.warning(f\"⚠️ No vectorizer found ⚠️\")\n",
    "\n",
    "# load classifier\n",
    "logging.info(f\"⚙️ Loading classifier...\")\n",
    "if os.path.exists(CLASSIFIER_URI):\n",
    "    with open(CLASSIFIER_URI, \"rb\") as f:\n",
    "        classifier = pickle.load(f)\n",
    "    logging.info(f\"✅ Classifier loaded\")\n",
    "else:\n",
    "    logging.warning(f\"⚠️ No classifier found ⚠️\")\n",
    "\n",
    "# load keep set (for preprocessing)\n",
    "logging.info(f\"⚙️ Loading keep set...\")\n",
    "if os.path.exists(KEEP_SET_URI):\n",
    "    with open(KEEP_SET_URI, \"rb\") as f:\n",
    "        keep_set = pickle.load(f)\n",
    "    logging.info(f\"✅ Keep set loaded\")\n",
    "else:\n",
    "    logging.warning(f\"⚠️ No keep set found ⚠️\")\n",
    "\n",
    "# load keep set (for preprocessing)\n",
    "logging.info(f\"⚙️ Loading exclude set...\")\n",
    "if os.path.exists(EXCLUDE_SET_URI):\n",
    "    with open(EXCLUDE_SET_URI, \"rb\") as f:\n",
    "        exclude_set = pickle.load(f)\n",
    "    logging.info(f\"✅ Exclude set loaded\")\n",
    "else:\n",
    "    logging.warning(f\"⚠️ No exclude set found ⚠️\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_input_title = \"pandas merge with Python >3.5\"\n",
    "usr_input_body = \"\"\"How can I perform a (INNER| (LEFT|RIGHT|FULL) OUTER) JOIN with pandas?\n",
    "How do I add NaNs for missing rows after a merge? How do I get rid of NaNs after merging?\n",
    "Can I merge on the index? How do I merge multiple DataFrames?\n",
    "I've seen these recurring questions asking about various facets of the pandas merge functionality, the aim here is to collate some of the more important points for posterity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jl/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jl/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user input: pandas merge with Python >3.5\n",
      "How can I perform a (INNER| (LEFT|RIGHT|FULL) OUTER) JOIN with pandas?\n",
      "How do I add NaNs for missing rows after a merge? How do I get rid of NaNs after merging?\n",
      "Can I merge on the index? How do I merge multiple DataFrames?\n",
      "I've seen these recurring questions asking about various facets of the pandas merge functionality, the aim here is to collate some of the more important points for posterity.\n",
      "\n",
      "\n",
      "clean input: pandas merge python perform inner| left|right|full outer join pandas add nan missing row merge get rid nan merging merge index merge multiple dataframes seen recurring asking various facet pandas merge functionality aim collate important point posterity\n"
     ]
    }
   ],
   "source": [
    "usr_input = usr_input_title + \"\\n\" + usr_input_body\n",
    "\n",
    "# 🚧 MAKE FUNCTION TO CHECK INPUT FIRST (data must be at least 2 words long, not punctuation:\n",
    "# preciser \"too many frequent words\" ou \"balises HTML supprimées\" ou \"modèle entraîné sur de l'anglais\"...\n",
    "\n",
    "# def preprocess_doc(document, keep_set, exclude_set) -> str:\n",
    "#     🚧 packages used -> re, nltk\n",
    "#     🚧 regrouper fonctions en une seule\n",
    "#     🚧 include keep_set and exclude_set in function\n",
    "#     doc_clean = clean_string(document)\n",
    "#     doc_tokens = tokenize_str(doc_clean, keep_set, exclude_set)\n",
    "#     doc_lemmed = lemmatize_tokens(doc_tokens, keep_set, exclude_set)\n",
    "#     doc_tk_clean = clean_tokens(doc_lemmed, keep_set, exclude_set)\n",
    "#     doc_preprocessed = \" \".join(doc_tk_clean)\n",
    "\n",
    "#     return doc_preprocessed\n",
    "\n",
    "input_clean = preprocess_doc(usr_input, keep_set, exclude_set)\n",
    "\n",
    "# 🚧 supprimer les outputs : les supprimer des fonctions\n",
    "print(\"user input:\", usr_input)\n",
    "print(\"\\nclean input:\", input_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50)\n",
      "[ 0.24701661  0.07568936  0.00817247  0.17902239 -0.3862364  -0.12290046\n",
      "  0.77592105  0.67494017 -0.6486079  -0.18690905  0.6684604  -0.29240564\n",
      " -0.5170923  -0.07515918  0.29629079  0.3047732   0.06215124 -0.14672922\n",
      " -0.22475429  0.44185334 -0.10042778 -0.04612633  0.6718335  -0.8425775\n",
      "  0.62273175  0.4572078   0.6682157   0.0265869  -0.27166218  0.36964867\n",
      "  0.4941888  -0.20693408 -0.4337857  -0.53141826 -0.24924181 -0.311\n",
      " -0.30004004  0.15352508  0.3684115  -0.4648135   0.26534975  0.7629548\n",
      " -0.5578143   0.15624113 -0.13686384  0.02493998 -0.06367211 -0.18456039\n",
      " -0.56151146  0.88944703]\n"
     ]
    }
   ],
   "source": [
    "X_vect = w2v_vect_data(vectorizer, [input_clean.split(\" \")])\n",
    "print(X_vect.shape)\n",
    "print(X_vect[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: pandas python dataframe github git\n"
     ]
    }
   ],
   "source": [
    "# predicted_probas = classifier.predict_proba(X_vect)\n",
    "lr_preds = lr_predict_tags(classifier, X_vect)\n",
    "predictions = str.join(\" \", lr_preds)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
